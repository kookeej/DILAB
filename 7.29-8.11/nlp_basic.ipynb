{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_basic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPD34+fXSFQEwXIlg7t4eT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kookeej/DILAB/blob/main/7.29-8.11/NLP_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz7RaZDSOqhx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LTDKAO9EYEt"
      },
      "source": [
        "자연어 처리 기초\n",
        "===\n",
        ">### Reference\n",
        ">* https://www.youtube.com/watch?v=2e9wnwuAVv0\n",
        ">* https://wikidocs.net/31698\n",
        "\n",
        "\n",
        "# 1. 텍스트 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSsxEszcEbzc"
      },
      "source": [
        "s = 'No pain no gain'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMYKGLwIE5gs",
        "outputId": "cbddaafd-894c-4d9e-b66f-dfd072298174"
      },
      "source": [
        "'pain' in s"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sip4SbGDFSM0",
        "outputId": "62d71814-b720-4fda-913f-af93d675f5fe"
      },
      "source": [
        "s.split()       # 공백을 기준으로 split"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No', 'pain', 'no', 'gain']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJdJfcyVFVsE",
        "outputId": "202dd1cb-9860-4815-850b-7106838be0f0"
      },
      "source": [
        "s.split().index('gain')         # split을 기준으로 'gain'이 몇 번째 인덱스에 있는지 확인하고 싶을 때"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D-BbFR65FoPE",
        "outputId": "dd7d4f36-de5c-4d93-bdeb-8a6af371299c"
      },
      "source": [
        "s[-4:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gain'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cJ2ZYazEF24z",
        "outputId": "4e3ce0aa-5800-4f99-c3c0-1edd06008f3e"
      },
      "source": [
        "s.split()[2][::-1]      # 'no'를 reverse"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'on'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_nWc7U8GEsl"
      },
      "source": [
        "s = \"한글도 처리 가능\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmSVK_HDGGjs",
        "outputId": "0af53ec3-e797-49df-90ee-83b15aacb63a"
      },
      "source": [
        "\"처리\" in s"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeUD7gqrGPFj"
      },
      "source": [
        "***\n",
        "# 2. 영어 처리\n",
        "한글과 영어는 둘 다 자연어긴 하지만 처리하는 방식이 매우 다르다. 따라서 별도로 구분해서 처리할 필요가 있다.    \n",
        "\n",
        "영어는 기본적으로 대소문자를 통합해 주어야 한다.\n",
        ">* 대소문자를 통합하지 않는다면 컴퓨터는 같은 언어를 다르게 받아들인다.\n",
        ">* 파이썬의 내장함수 ```lower()```, ```upper()```를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXRz2eDgGsZg",
        "outputId": "bea551ae-a602-4519-f428-b39bfcc7f1a3"
      },
      "source": [
        "s = \"This Is A Sentence\"\n",
        "str_lower = s.lower()\n",
        "str_upper = s.upper()\n",
        "print(str_lower, str_upper)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is a sentence THIS IS A SENTENCE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGDyxR4BHAnd"
      },
      "source": [
        "***\n",
        "# 3. 정규화(Normalization)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgEM6F6GHNET",
        "outputId": "50a46140-3104-426b-8a58-e77339d369bb"
      },
      "source": [
        "s = \"I visited UK from US on 22-09-20\"\n",
        "print(s)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I visited UK from US on 22-09-20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4FT3D69HTyw"
      },
      "source": [
        "우리는 변환 과정을 통일성 있게 가져야할 필요가 있다. 예를 들어 위 코드의 'UK', 'US'같은 경우, 통일된 명칭으로 구성할 필요가 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ZbR7QcKmAr",
        "outputId": "fb34ad76-8a4a-4c2d-b641-695c763fb96a"
      },
      "source": [
        "new_s = s.replace(\"UK\", \"United Kingdom\").replace(\"US\", \"United States\").replace(\"-20\", \"-2020\")\n",
        "print(new_s)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I visited United Kingdom from United States on 22-09-2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob1RwKk8OvXX"
      },
      "source": [
        "## 정규 표현식\n",
        "데이터 전처리에서 정규 표현식을 아주 많이 사용한다.\n",
        "* 정규 표현식은 특정 문자들을 편리하게 지정하고 추가, 삭제가 가능하다.\n",
        "* 파이썬에서는 정규 표현식을 지원하는 ```re```패키지를 제공한다.    \n",
        "\n",
        "|특수문자|설명|\n",
        "|----|---------------------------------|\n",
        "|```.```|앞의 문자 한 개를 표현|\n",
        "|```?```|앞 문자 한 개를 표현하나 존재할 수도, 존재하지 않을 수도 있음(0개 또는 1개)|\n",
        "|```*```|앞의 문자가 0개 이상|\n",
        "|```+```|앞의 문자가 최소 1개 이상|\n",
        "|```^```|뒤의 문자로 문자열이 시작|\n",
        "|```\\$```|앞의 문자로 문자열이 끝남|\n",
        "|```\\{n\\}```|n번만큼 반복|\n",
        "|```\\{n1, n2\\}```|n1 이상, n2 이하만큼 반복. n2를 지정하지 않으면 n1 이상만 반복|\n",
        "|```\\[ abv \\]```|안에 문자들 중 한 개의 문자와 매치. a-z처럼 범위도 지정 가능|\n",
        "|```\\[ ^a \\]```|해당 문자를 제외하고 매치|\n",
        "|```aㅣb```|a 또는 b를 나타냄|    \n",
        "\n",
        "* 정규 표현식에서 자주 사용하는 역 슬래시를 이용한 문자 규칙    \n",
        "\n",
        "|특수문자|설명|\n",
        "|----|---------------------------------|\n",
        "|```\\\\```|역슬래시 자체를 의미|\n",
        "|```\\d```|모든 숫자를 의미, [0-9]와 동일|\n",
        "|```\\D```|숫자를 제외한 모든 문자들을 의미, [^0-9]와 동일|\n",
        "|```\\s```|공백을 의미. [\\t\\n\\r\\f\\v]와 동일|\n",
        "|```\\S```|공백을 제외한 모든 문자를  의미. [^ \\t\\n\\r\\f\\v]와 동일|\n",
        "|```\\w```|문자와 숫자를 의미. [a-zA-Z0-9]와 동일|\n",
        "|```\\W}```|문자와 숫자를 제외한 다른 문자를 의미. [^a-zA-Z0-9]와 동일|    \n",
        "\n",
        "\n",
        "## Match\n",
        "컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사한다. 정규 표현식을 사용하기 위해 ```re``` 패키지를 import해준다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVhjUSkKVXCM",
        "outputId": "40dae1ab-f7de-4dd9-b9cb-fa8d171913c3"
      },
      "source": [
        "import re\n",
        "\n",
        "check = 'ab.'           # . 부분에 반드시 한 문자가 와야한다.\n",
        "\n",
        "print(re.match(check, 'abc'))       # Match\n",
        "print(re.match(check, 'c'))         # None\n",
        "print(re.match(check, 'ab'))        # None"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9XsXlCHWLXU"
      },
      "source": [
        "## Compile\n",
        "* compile을 사용할 경우에, 여러 번 사용할 경우 일반적인 사용보다 더 빠르게 사용할 수 있다.\n",
        "* compile을 통해 정규 표현식을 사용할 경우에는 ```re```가 아닌 컴파일한 객체 이름을 통해 사용해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js0f7W7HWeQ0",
        "outputId": "2f7e5821-8d74-45aa-bb01-a9461a50caf4"
      },
      "source": [
        "import time     # 시간 체크\n",
        "\n",
        "normal_s_time = time.time()\n",
        "r = 'ab.'\n",
        "for i in range(1000):\n",
        "    re.match(check, 'abc')\n",
        "print('일반 사용 시 소요시간:', time.time() - normal_s_time)\n",
        "\n",
        "compile_s_time = time.time()\n",
        "r = re.compile('ab.')\n",
        "for i in range(1000):\n",
        "    r.match(check)\n",
        "print('컴파일 사용 시 소요시간:', time.time() - compile_s_time)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "일반 사용 시 소요시간: 0.001035451889038086\n",
            "컴파일 사용 시 소요시간: 0.0003609657287597656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHBtTeT3YkdD"
      },
      "source": [
        "## Search\n",
        "search는 match와는 다르게 문자열의 전체를 검사한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CBIYM8LYtVR",
        "outputId": "28aae9a9-1544-4ec1-9b3c-16694e91f433"
      },
      "source": [
        "check = 'ab?'       # a 그리고 1개 또는 0개의 b\n",
        "\n",
        "print(re.search('a', check))        # a만 올 수도 있다.\n",
        "print(re.match('kkkab', check))\n",
        "print(re.search('kkkab', check))\n",
        "print(re.match('ab', check))        # "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 1), match='a'>\n",
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 2), match='ab'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUYhHTsXZtjT"
      },
      "source": [
        "## Split\n",
        "정규 표현식에 해당하는 문자열을 기준으로 문자열을 나눈다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvSH5aFlZxkK",
        "outputId": "e95188c9-03a1-4c8d-d779-eb9e88709392"
      },
      "source": [
        "r = re.compile(' ')\n",
        "print(r.split('abc abbc abcabc'))\n",
        "\n",
        "r = re.compile('c')\n",
        "print(r.split('abc abbc abcabc'))\n",
        "\n",
        "r = re.compile('[1-9]')\n",
        "print(r.split('s1abc 2v3s 4sss 5a'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abc', 'abbc', 'abcabc']\n",
            "['ab', ' abb', ' ab', 'ab', '']\n",
            "['s', 'abc ', 'v', 's ', 'sss ', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opnIgKmfad9D"
      },
      "source": [
        "## Sub\n",
        "정규 표현식과 일치하는 부분을 다른 문자열로 교체한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-8xVZc_ahsT",
        "outputId": "039835bf-5f31-40c7-a55e-7cebfd985359"
      },
      "source": [
        "print(re.sub('[a-z]', 'abcdefg', '1'))\n",
        "\n",
        "print(re.sub('[^a-z]', 'abc defg', '1'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "abc defg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJ_LIHabD45"
      },
      "source": [
        "## findall\n",
        "컴파일한 정규 표현식을 이용하여 정규 표현식과 맞는 모든 문자(열)을 리스트로 반환시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zXvXNrNb1wb",
        "outputId": "04ecc5d0-ff52-4483-e252-aa838d0c2fa4"
      },
      "source": [
        "print(re.findall('[\\d]',  '1ab 2cd 3ef 4g'))    # 숫자\n",
        "\n",
        "print(re.findall('[\\W]', '!abcd@@#'))           # 문자와 숫자가 아닌 특수문자"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '2', '3', '4']\n",
            "['!', '@', '@', '#']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgK2SijQcQAN"
      },
      "source": [
        "## finditer\n",
        "컴파일한 정규 표현식을 이용하여 정규 표현식과 맞는 모든 문자(열)을 ```iterator``` 객체로 반환한다. ```iterator``` 객체를 이용하면 생성된 객체를 하나씩 자동으로 가져올 수 있어서 처리가 편리하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeJXwo0Ccdxz",
        "outputId": "2f4666db-8315-4941-cd3e-4da56e5cf86c"
      },
      "source": [
        "iter1 = re.findall('[\\d]',  '1ab 2cd 3ef 4g')\n",
        "print(iter1)\n",
        "for i in iter1:\n",
        "    print(i)\n",
        "\n",
        "iter2 = re.findall('[\\W]', '!abcd@@#')\n",
        "print(iter2)\n",
        "for i in iter2:\n",
        "    print(i)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '2', '3', '4']\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "['!', '@', '@', '#']\n",
            "!\n",
            "@\n",
            "@\n",
            "#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR7UQ8p3dB5a"
      },
      "source": [
        "***\n",
        "# 4. 토큰화(Tokenization)\n",
        "주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부른다.\n",
        "\n",
        "* 특수문자에 대한 처리\n",
        ">* 알파벳, 숫자와는 달리 특수문자는 별도의 처리가 필요하다.\n",
        ">* 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만, 특수문자가 단어에 특별한 의미를 가질 때 이를 학습에 반영시키지 못할 수도 있다.\n",
        ">* 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고 처리를 하는 것이 더 중요하다.    \n",
        "\n",
        "\n",
        "* 특정 단어에 대한 토큰 분리 방법\n",
        ">* 한 단어지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 'we're'나 'United Kingdom'과 같은 단어는 어떻게 분리해야 할지 선택해야 한다.\n",
        ">* we're은 한 단어지만 분리해도 단어의 의미에 별 영향을 끼치지 않지만 'United Kingdom'은 두 단어가 모여 특정 의미를 가리키기 때문에 분리시켜서는 안된다.\n",
        ">* 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리하다.    \n",
        "\n",
        "\n",
        "## 단어 토큰화\n",
        "파이썬의 내장 함수인 ```split```을 사용하여 단어를 토큰화한다. 보통 공백을 기준으로 단어를 분리한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI2wcKxMf31L",
        "outputId": "069c97b3-f7ba-4a43-bb9e-657c692bbbf0"
      },
      "source": [
        "sentence = 'Time is gold'\n",
        "tokens = [x for x in sentence.split(' ')]\n",
        "tokens"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'gold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzrCT4BZgArJ"
      },
      "source": [
        "하지만 ```nltk``` 패키지를 사용하여 손쉽게 토큰화를 구현할 수 있다. ```tokenize``` 모듈을 사용한다. 단어 토큰화는 ```word_tokenize()```함수를 사용하여 구현 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCeH1OITgPVy",
        "outputId": "5aec0d3c-181e-45f4-8303-3e62a26a41dd"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm1EaGdEggph",
        "outputId": "6c11b480-b491-4c4a-d322-1f6f7616c3fe"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'gold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD2ppZ3rgojb"
      },
      "source": [
        "좀 더 간결하게 토큰화하는 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCG0JaWvg0Nb"
      },
      "source": [
        "## 문장 토큰화\n",
        "문장 토큰화는 줄바꿈 문자```\\n```를 기준으로 문장을 분리한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tatl2S8JhXBC",
        "outputId": "034b4769-5ba0-4945-a5e9-6b7070dab165"
      },
      "source": [
        "sentences = 'The world is a beautiful book.\\nBut of litte use to him who cannot read it.'\n",
        "print(sentences)\n",
        "\n",
        "tokens = [x for x in sentences.split('\\n')]\n",
        "tokens"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The world is a beautiful book.\n",
            "But of litte use to him who cannot read it.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.',\n",
              " 'But of litte use to him who cannot read it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_E81PQlhox4"
      },
      "source": [
        "문장 토큰화는 ```sent_tokenize()``` 함수를 사용하여 구현이 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTqA1FrrhvN7",
        "outputId": "c03833a3-915e-4c1c-c810-eeb65238289d"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "tokens = sent_tokenize(sentences)\n",
        "tokens"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.',\n",
              " 'But of litte use to him who cannot read it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9z8hWZph7Gx"
      },
      "source": [
        "문장 토큰화는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있다. 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더욱 좋은 토큰화를 구현할 수도 있다.    \n",
        "\n",
        "## 정규 표현식을 이용한 토큰화\n",
        "토큰화 기능을 직접 구현할 수도 있지만 정규 표현식을 이용하여 간단하게 구현할 수도 있다. ```nltk``` 패키지는 정규 표현식을 사용하는 토큰화 도구인 ```RegexpTokenizer```를 제공한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ2i4mg1iYOa",
        "outputId": "b8cb839e-6bb4-42fc-935b-1e0fd31ceadb"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "tokenizer = RegexpTokenizer('[\\w]+')        # 문자와 숫자가 앞에 최소 1개 이상 있어야 한다.\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvyRXhQGj6VA",
        "outputId": "627ff525-3af2-4521-af4a-20827f4090d4"
      },
      "source": [
        "tokenizer = RegexpTokenizer('[\\s]+', gaps=True)     # 특수문자를 남기고 공백을 기준으로 토큰화한다.\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LgMjVtvlCI7"
      },
      "source": [
        "특수문자가 전부 포함된 상태로 리스트에 저장되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MI90De1lTGp"
      },
      "source": [
        "## 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-10Z3I-_lVxw",
        "outputId": "b6f01b81-2c9e-439b-bc3b-147c5cba7aee"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "\n",
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYalEYxJlhZg"
      },
      "source": [
        "공백 기준으로 토큰화된 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNQ_4Zeelkao"
      },
      "source": [
        "## TextBlob을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f8ubz0ilnoV",
        "outputId": "a1473719-ecf1-43d1-f412-c4de74582575"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = 'Where there\\'s a will, there\\'s a way'\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P3DyILPl2bS"
      },
      "source": [
        "```'s``` 이렇게 떼어냈다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQA74u_nl8uY"
      },
      "source": [
        "## 기타 Tokenizer\n",
        "* ```WhiteSpaceTokenizer```: 공백을 기준으로 토큰화\n",
        "* ```WordPunktTokenizer```: 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화\n",
        "* ```MWETokentizer```: MWE는 Multi-Word Expression의 약자로 'republic of korea'와 같이 여러 단어로 이뤄진 특정 그룹을 한 개체로 취급한다.\n",
        "* ```TweetTokenizer```: 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룬다.    \n",
        "\n",
        "\n",
        "## n-gram 추출\n",
        "n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석한다. n=1일 때는 unigram, n=2일 때는 bigram, n=3일 때는 trigram으로 불린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIfQR6anmwhI",
        "outputId": "adc0abec-e270-453b-df96-f9cab28359cc"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'There is no royal road to learning'\n",
        "bigram = list(ngrams(sentence.split(), 2))\n",
        "print(bigram)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3zIU8VKmwuf",
        "outputId": "3f9fddd1-ec7f-4930-e896-bf1c6fbb2390"
      },
      "source": [
        "trigram = list(ngrams(sentence.split(), 3))\n",
        "print(trigram)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'learning')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmdicScmnS6J"
      },
      "source": [
        "```nltk```의 ```ngrams```뿐만 아니라 ```textblob```을 사용해도 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuqsPSz-mw4o",
        "outputId": "6ace5dfe-b50f-4044-e3ec-7d64d4e4bb3a"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "\n",
        "# bigram\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is']),\n",
              " WordList(['is', 'no']),\n",
              " WordList(['no', 'royal']),\n",
              " WordList(['royal', 'road']),\n",
              " WordList(['road', 'to']),\n",
              " WordList(['to', 'learning'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51VbICRjmxC4",
        "outputId": "56b8bc91-94a9-4939-9ab2-7aa008048f85"
      },
      "source": [
        "# trigram\n",
        "blob.ngrams(n=3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is', 'no']),\n",
              " WordList(['is', 'no', 'royal']),\n",
              " WordList(['no', 'royal', 'road']),\n",
              " WordList(['royal', 'road', 'to']),\n",
              " WordList(['road', 'to', 'learning'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-b7Zchanv2-"
      },
      "source": [
        "***\n",
        "# 5.PoS 태깅 (Parts of Speech Tagging)\n",
        "```PoS```는 품사를 의미한다. ```PoS Tagging```은 문장 내에서 단어에 해당하는 각 품사를 Tagging한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hTu-oUgoIi_"
      },
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLxWz6uoJRy",
        "outputId": "6cea5979-aa53-4e05-a72d-590988fae0a3"
      },
      "source": [
        "words = word_tokenize(\"Think like man of action and act like man of thought\")\n",
        "words"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Think',\n",
              " 'like',\n",
              " 'man',\n",
              " 'of',\n",
              " 'action',\n",
              " 'and',\n",
              " 'act',\n",
              " 'like',\n",
              " 'man',\n",
              " 'of',\n",
              " 'thought']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HIzu9zDoJLC",
        "outputId": "0b234a5f-c771-4aa5-8222-3fce4d9e83aa"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(words)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Think', 'VBP'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('action', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('act', 'NN'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('thought', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKMKRqxPo3J9"
      },
      "source": [
        "각 단어의 품사에 대한 태그가 붙었다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WLufu0SoI-T",
        "outputId": "a474a7fc-7135-45e6-d0c6-1f2bca59ea26"
      },
      "source": [
        "nltk.pos_tag(word_tokenize(\"A rolling stone gathers no moss\"))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 'DT'),\n",
              " ('rolling', 'VBG'),\n",
              " ('stone', 'NN'),\n",
              " ('gathers', 'NNS'),\n",
              " ('no', 'DT'),\n",
              " ('moss', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r13wMOZWpJ_v"
      },
      "source": [
        "### PoS Tag List    \n",
        "\n",
        "| Number | Tag | Description | 설명 |\n",
        "| -- | -- | -- | -- |\n",
        "| 1 | `CC` | Coordinating conjunction |\n",
        "| 2 | `CD` | Cardinal number |\n",
        "| 3 | `DT` | Determiner | 한정사\n",
        "| 4 | `EX` | Existential there |\n",
        "| 5 | `FW` | Foreign word | 외래어 |\n",
        "| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |\n",
        "| 7 | `JJ` | Adjective | 형용사 |\n",
        "| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |\n",
        "| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |\n",
        "| 10 | `LS` | List item marker |\n",
        "| 11 | `MD` | Modal |\n",
        "| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |\n",
        "| 13 | `NNS` | Noun, plural | 명사, 복수형 |\n",
        "| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |\n",
        "| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |\n",
        "| 16 | `PDT` | Predeterminer | 전치한정사 |\n",
        "| 17 | `POS` | Possessive ending | 소유형용사 |\n",
        "| 18 | `PRP` | Personal pronoun | 인칭 대명사 |\n",
        "| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |\n",
        "| 20 | `RB` | Adverb | 부사 |\n",
        "| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |\n",
        "| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |\n",
        "| 23 | `RP` | Particle |\n",
        "| 24 | `SYM` | Symbol | 기호\n",
        "| 25 | `TO` | to |\n",
        "| 26 | `UH` | Interjection | 감탄사 |\n",
        "| 27 | `VB` | Verb, base form | 동사, 원형 |\n",
        "| 28 | `VBD` | Verb, past tense | 동사, 과거형 |\n",
        "| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |\n",
        "| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |\n",
        "| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |\n",
        "| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |\n",
        "| 33 | `WDT` | Wh-determiner |\n",
        "| 34 | `WP` | Wh-pronoun |\n",
        "| 35 | `WP$` | Possessive wh-pronoun |\n",
        "| 36 | `WRB` | Wh-adverb |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOLqciX2rMfe"
      },
      "source": [
        "***\n",
        "# 5. 불용어 제거\n",
        "불용어 제거란, **영어의 전치사** 또는 **한글의 조사**와 같이 의미에 영향을 주지 않는 것들을 제거하는 것을 말한다.\n",
        "* 길이가 짧은 단어, 등장 빈도 수가 적은 단어들도 분석에 큰 영향을 주지 않는다.\n",
        "* 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만 완벽하게 제거되지는 않는다.\n",
        "* 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋다.\n",
        "* 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전을 만들어 불필요한 단어들을 제거한다.    \n",
        "\n",
        "영어로는 ```stop word```라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWs59qwKryKH",
        "outputId": "3c3aec4c-6afa-429e-d858-ed6d3c10fa9e"
      },
      "source": [
        "stop_words = 'on in the'\n",
        "stop_words = stop_words.split(' ')\n",
        "stop_words"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['on', 'in', 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBwX70bVr5du",
        "outputId": "3577760b-6e8f-4932-be94-4690b6dbf960"
      },
      "source": [
        "sentence = 'singer on the stage'\n",
        "sentence = sentence.split(' ')\n",
        "nouns = []\n",
        "for noun in sentence:\n",
        "    if noun not in stop_words:\n",
        "        nouns.append(noun)\n",
        "\n",
        "nouns"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['singer', 'stage']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZF_1rJusP5t"
      },
      "source": [
        "```nltk``` 패키지의 불용어 리스트를 사용해도 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h77L-B-UsT8A",
        "outputId": "aa576d9d-4e2b-4caa-dcff-e527de2bc4bb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzC_8vlasgy1",
        "outputId": "4d30885c-f092-44fa-b76a-70a2b6bc546d"
      },
      "source": [
        "stop_words = stopwords.words('english')         # 영어 기준 불용어를 가져온다.\n",
        "\n",
        "print(stop_words)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spp0pFUashD2",
        "outputId": "dddf0087-ab40-4756-d548-6ca711884b87"
      },
      "source": [
        "s = 'If you do not walk today, you will have to run tomorrow.'\n",
        "words = word_tokenize(s)\n",
        "print(words)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['If', 'you', 'do', 'not', 'walk', 'today', ',', 'you', 'will', 'have', 'to', 'run', 'tomorrow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzjyR_LtshR0",
        "outputId": "8c796e10-74e0-435d-eb7e-8e0eea56bc81"
      },
      "source": [
        "no_stopwords = []\n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "        no_stopwords.append(w)\n",
        "\n",
        "print(no_stopwords)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['If', 'walk', 'today', ',', 'run', 'tomorrow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuiqI41zuYtt"
      },
      "source": [
        "***\n",
        "# 6. 그외 언어 처리\n",
        "## 철자 교정\n",
        "텍스트에 오탈자가 존재하는 경우가 있다. 예를 들어 단어 'apple'를 'aplpe'과 같이 철자 순서가 바뀌거나 'spple'처럼 철자가 틀릴 수도 있다. 컴퓨터는 이러한 오류를 그대로 받아들이기 때문에 별도의 처리가 필요하다.     \n",
        "\n",
        "철자 교정 알고리즘은 이미 개발되어 워드 프로세서나 다양한 서비스에서 많이 적용되고 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa_Euj-quxOB",
        "outputId": "351b0ce7-4bf7-4719-9147-0b1857ef710b"
      },
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
            "\u001b[K     |████████████████████████████████| 622 kB 4.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621853 sha256=1e21e66e14be1e1d3744a47565d947f1bb88d84dcea04e22e94af5a9ddb668c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/8e/bd/f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdB_dIgXuxdm",
        "outputId": "3a87826c-e5f9-406a-97ae-a36f4129ebec"
      },
      "source": [
        "spell = Speller('en')       # 영어로 체크\n",
        "print(spell('peoplle'))\n",
        "print(spell('peeple'))\n",
        "print(spell('peope'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "people\n",
            "people\n",
            "people\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALHFOEhauxw2",
        "outputId": "2a306132-8e72-476a-f589-c9ca273f41d3"
      },
      "source": [
        "s = word_tokenize(\"Earlly vird catchess the womm.\")\n",
        "print(s)\n",
        "ss = ' '.join([spell(s) for s in s])\n",
        "print(ss)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Earlly', 'vird', 'catchess', 'the', 'womm', '.']\n",
            "Early bird catches the worm .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_odWW2GWvshf"
      },
      "source": [
        "## 언어의 단수화와 복수화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwV3IMM6vxf3",
        "outputId": "43090650-2eec-4243-be19-a41c4875f799"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "words = 'apples bananas oranges'\n",
        "tb = TextBlob(words)\n",
        "\n",
        "print(tb.words)\n",
        "print(tb.words.singularize())           # 단수화"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apples', 'bananas', 'oranges']\n",
            "['apple', 'banana', 'orange']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1p2h2rVvxpP",
        "outputId": "30333614-3e75-4247-e3d2-c342f0f653d1"
      },
      "source": [
        "words = 'car train airplane'\n",
        "tb = TextBlob(words)\n",
        "\n",
        "print(tb.words)\n",
        "print(tb.words.pluralize())         # 복수화"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['car', 'train', 'airplane']\n",
            "['cars', 'trains', 'airplanes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjEhKt-Wwg4t"
      },
      "source": [
        "## 어간(Stemming) 추출\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9PJnQ-mwme2"
      },
      "source": [
        "import nltk\n",
        "stemmer = nltk.stem.PorterStemmer()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m1xZn5cews5S",
        "outputId": "4f511a91-3005-48ce-adc1-6439519d1884"
      },
      "source": [
        "stemmer.stem('application')\n",
        "# application의 어간인 applic이 추출된다."
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'applic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BQ2ePFcNw1zx",
        "outputId": "0ec7e014-dc7d-4b10-963c-4eacf8c4503b"
      },
      "source": [
        "stemmer.stem('beginning')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'begin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UCFHhNG-w7l1",
        "outputId": "1bd9edc7-112b-4b93-f38a-c1311fdfaa71"
      },
      "source": [
        "stemmer.stem('catches')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HcYEzK8Kw_Sm",
        "outputId": "63e3f7b5-898c-4484-d198-182e848f6751"
      },
      "source": [
        "stemmer.stem('education')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'educ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buoOHXXQxCb0"
      },
      "source": [
        "## 표제어(Lemmatization) 추출\n",
        "표제어란, 사전에 등재된 단어를 말한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsLSGjpfxF6e",
        "outputId": "1f9fd290-ccc0-4551-be29-25e6915b41ad"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t42PUgHgxN_m",
        "outputId": "2f851704-f7f2-4838-8d7e-a090e76cdc91"
      },
      "source": [
        "lemmatizer.lemmatize('application')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'application'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5Cj7GtwPxZhe",
        "outputId": "5190c822-a04f-4b08-c487-ac6e0ee0b8a2"
      },
      "source": [
        "lemmatizer.lemmatize('beginning')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beginning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8jSuFOfnxg9N",
        "outputId": "1a8840dc-a869-4d6c-896b-403e1952be19"
      },
      "source": [
        "lemmatizer.lemmatize('catches')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CaofgIpBxo-N",
        "outputId": "c57d6151-8092-4d54-8001-45ab693c3e7e"
      },
      "source": [
        "lemmatizer.lemmatize('education')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'education'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_IXBriU_zwgl",
        "outputId": "7b09b63b-efb2-468a-8012-228cbb21c96f"
      },
      "source": [
        "lemmatizer.lemmatize('saw')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'saw'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USb0F_e3xrYl"
      },
      "source": [
        "## 개체명 인식 (Named Entity Recognition)\n",
        "말 그대로 이름을 가진 개체를 인식하겠다는 것을 의미한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-BaGVUbx-vn",
        "outputId": "eabd3767-2d54-4fd4-ccd3-b3a627391444"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llMRvmScyONJ",
        "outputId": "43e075a3-b2e6-484b-d70b-d121e33def2c"
      },
      "source": [
        "s = \"Rome was not built in a day\"\n",
        "print(s)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rome was not built in a day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sp6SpS9yTJ1",
        "outputId": "a9e6ea6e-22cf-4a2c-868b-3288e757e2a8"
      },
      "source": [
        "tags = nltk.pos_tag(word_tokenize(s))\n",
        "print(tags)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('built', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RuR8l17yZc1",
        "outputId": "eb2641c3-5f00-42a4-c444-b17923b91a31"
      },
      "source": [
        "entities = nltk.ne_chunk(tags, binary=True)\n",
        "print(entities)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qj7pm7Sym2b"
      },
      "source": [
        "'Rome' 앞에만 ```NE```가 붙었다. ```NE```는 Named Entity의 준말이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw_WGKnnyy54"
      },
      "source": [
        "## 단어 중의성 (Lexical Ambiguity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U13fMp_Yy4MF",
        "outputId": "0b2b20b4-6651-472f-bc97-24cfb6bd5bda"
      },
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "s = \"I saw bats.\"\n",
        "# 나는 톱으로 야구방망이들을 썰었다.\n",
        "# 나는 박쥐들을 보았다.\n",
        "# 나는 야구 방망이들을 보았다.\n",
        "# 나는 톱으로 박쥐들을 썰었다.\n",
        "\n",
        "print(word_tokenize(s))\n",
        "print(lesk(word_tokenize(s), 'saw'))\n",
        "print(lesk(word_tokenize(s), 'bats'))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'saw', 'bats', '.']\n",
            "Synset('saw.v.01')\n",
            "Synset('squash_racket.n.01')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIkNSRTzeFV"
      },
      "source": [
        "분석 결과, 'saw'는 동사의 의미로 보는 것이 좋다. 'bats'는 'squash_racket'으로 명사로 보는 것이 좋다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNd15TP-z0Cz"
      },
      "source": [
        "***\n",
        "# 7. 한국어 처리\n",
        "## 정규 표현식\n",
        "한국어 정규 표현식도 대부분의 문법은 영어 정규 표현식과 같다. 다만 한국어는 자음과 모음이 분리되어 있기 때문에 문법을 지정할 때는 자음과 모음을 동시에 고려해야 한다.    \n",
        "\n",
        "## match\n",
        "컴파일한 정규 포현식을 이용해 문자열이 정규 표현식과 맞는지 검사한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ5G6KS20MVj",
        "outputId": "5c81eb31-f0a2-4a71-cbfc-f34708391b43"
      },
      "source": [
        "import re\n",
        "\n",
        "check = '[ㄱ-ㅎ]+'\n",
        "\n",
        "print(re.match(check, 'ㅎ 안녕하세요.'))\n",
        "print(re.match(check, '안녕하세요.ㅎ'))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 1), match='ㅎ'>\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQURZjPB0drI"
      },
      "source": [
        "```ㅎ 안녕하세요.``` 의 경우에는 잘 매치되었다. 하지만 ```안녕하세요. ㅎ``` 는 매치가 안되었다. 왜 그럴까?    \n",
        "\n",
        "첫 번째 문장의 경우에는 ㅎ라는 자음 하나만 왔기 때문에 뒷 문장과 잘 매치가 되었다. 하지만 두 번째 문장의 경우에는 앞 문장이 자음과 모음이 함께 섞여있기 때문에 매칭이 되지 않는다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMY-AT4B1Vl7"
      },
      "source": [
        "## search\n",
        "match와 다르게 search는 문자열의 전체를 검사한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKAD8R8c1Z9q",
        "outputId": "c8d56d5d-c6e1-4a01-97cb-da2622e8a0a1"
      },
      "source": [
        "check = '[ㄱ-ㅎ|ㅏ-ㅣ]+'\n",
        "\n",
        "print(re.search(check, 'ㄱㅏ 안녕하세요'))\n",
        "print(re.match(check, '안 ㄱㅏ'))\n",
        "print(re.search(check, '안 ㄱㅏ'))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
            "None\n",
            "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaVzCluX9Z0D"
      },
      "source": [
        "## sub\n",
        "정규 표현식과 일치하는 부분을 다른 문자열로 교체한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWeuc1Cg-_86",
        "outputId": "d7678187-adec-4e18-e99d-887899bbf15c"
      },
      "source": [
        "print(re.sub('[가-힣]', '가나다라마바사', '1'))         # 모든 한글\n",
        "print(re.sub('[^가-힣]', '가나다라마바사', '1'))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "가나다라마바사\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNN8_Jsj_Yhf"
      },
      "source": [
        "## 토큰화 (Tokenization)\n",
        "한국어를 학습 데이터로 사용할 때는 언어의 특성으로 인해 추가로 고려해야할 사항들이 있다. \n",
        "* 한국어는 띄어쓰기를 준수하지 않아도 의미가 전달되는 경우가 많아 띄어쓰기가 지켜지지 않을 가능성이 존재한다.\n",
        "* 띄어쓰기가 지켜지지 않으면 정상적인 토큰 분리가 이루어지지 않는다.\n",
        "* 한국어는 형태소라는 개념이 존재해 그 개념을 추가로 고려해주어야 한다.\n",
        "* '그는', '그가' 등의 단어들은 같은 의미를 가리키지만 텍스트 처리에서는 다르게 받아들일 수 있어 처리를 해줘야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF1NRNdyAuFt"
      },
      "source": [
        ">### 한국어 자연어 처리를 위한 ```konlpy```와 형태소 분석기 ```MeCab```을 설치!\n",
        "* https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qphjaMiBDDt"
      },
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emtf4ZlHBVy4"
      },
      "source": [
        "## 단어 토큰화\n",
        "한국어는 공백으로 단어를 분리해도 조사, 접속사 등이 남아 분석이 어렵다. 한국어 토큰화는 조사, 접속사를 분리하거나 제거함으로써 이 문제를 해결해준다.    \n",
        "\n",
        "한국어 토큰화를 사용하기 위해서는 ```konlpy```와 ```macab``` 이라는 라이브러리가 필요하다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUg_ODc4B4GE"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab()"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjnJsJLEB4VR",
        "outputId": "451dc7ef-5e8e-4551-b28c-ff701fb97b7f"
      },
      "source": [
        "sentence = '언제나 현재에 집중할 수 있다면 행복할 것이다.'\n",
        "tagger.pos(sentence)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('언제나', 'MAG'),\n",
              " ('현재', 'NNG'),\n",
              " ('에', 'JKB'),\n",
              " ('집중', 'NNG'),\n",
              " ('할', 'XSV+ETM'),\n",
              " ('수', 'NNB'),\n",
              " ('있', 'VV'),\n",
              " ('다면', 'EC'),\n",
              " ('행복', 'NNG'),\n",
              " ('할', 'XSV+ETM'),\n",
              " ('것', 'NNB'),\n",
              " ('이', 'VCP'),\n",
              " ('다', 'EF'),\n",
              " ('.', 'SF')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfPlCs_wB5Yu"
      },
      "source": [
        "토큰화만 실행할 때는 ```tagger.morphs()```라는 함수를 이용한다. 형태소를 분석한 토큰화된 결과만 띄운다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2TnYCFVCP4D",
        "outputId": "7ab0ca38-1cbd-4b29-eb75-15e4b4d8bacb"
      },
      "source": [
        "tagger.morphs(sentence)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['언제나', '현재', '에', '집중', '할', '수', '있', '다면', '행복', '할', '것', '이', '다', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NwBrG-SCbj1"
      },
      "source": [
        "형태소만 출력하고 싶을 경우에는 ```tagger.nouns()```라는 함수를 사용하여 조사, 접속사 등을 제거할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFbno6aICiLR",
        "outputId": "2009594d-7989-419a-97f0-3221a06ed480"
      },
      "source": [
        "tagger.nouns(sentence)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['현재', '집중', '수', '행복', '것']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5e1PL3NDB3p"
      },
      "source": [
        "## 문장 토큰화\n",
        "한국어 문장을 토큰화할 때는 ```kss```(korean sentence splitter) 라이브러리를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmizM_A3DLSe",
        "outputId": "d6d10417-5252-434b-80c5-bba71042b267"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-2.5.1-py3-none-any.whl (65 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 40 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 65 kB 2.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: kss\n",
            "Successfully installed kss-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ywNyz0UDRdg"
      },
      "source": [
        "* 아무리 다른 좋은 라이브러리를 사용하더라도 한국어에는 전치 표현이 존재하기 때문에 제대로 토큰화가 되지 않는다.\n",
        "* 좀 더 나은 학습을 위해 사용자는 해당 부분을 따로 처리해주어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlruJI-WDaaq",
        "outputId": "8b3e45f9-d56c-4236-f094-907e57bde7b8"
      },
      "source": [
        "import kss\n",
        "text = '진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어? 나는...'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['진짜? 내일 뭐하지.', '이렇게 애매모호한 문장도? 밥은 먹었어?', '나는...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RSXi_7zDnod"
      },
      "source": [
        "## 정규 표현식을 이용한 토큰화\n",
        "한국어도 정규 표현식을 이용해 토큰화가 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0ElqXL0Dsq2",
        "outputId": "507466f2-5c19-469a-d7b7-f07643e18bd8"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = '안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.'\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiFSISv1EHn8"
      },
      "source": [
        "불필요한 부분들이 빠진 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqPSWHJ-F7xL",
        "outputId": "cf7e8175-d81d-4237-da85-8299f9bc2a9c"
      },
      "source": [
        "# 자음 기준\n",
        "tokenizer = RegexpTokenizer(\"[ㄱ-ㅎ]+\", gaps=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['안녕하세요 ', ' 저는 자연어 처리(Natural Language Processing)를', '!! 배우고 있습니다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvOrXumeGOBX"
      },
      "source": [
        "자음을 기준으로 토큰화된 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TupUf-CGUmG"
      },
      "source": [
        "## Keras를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mnbPEESGWmw",
        "outputId": "2ef7524d-6d30-48a4-d91c-80a8d179d14a"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = '성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'\n",
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUU-nOVyGg7_"
      },
      "source": [
        "## TextBlob을 이용한 토큰화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BupRvNmzGkG2",
        "outputId": "1eb8e122-4eed-4b76-be1f-f7d7537b5a0f"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfk4PqONGwrP"
      },
      "source": [
        "***\n",
        "# 8. Bag of Words(BoW)\n",
        "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법이다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odpfVNmsHhhF",
        "outputId": "0345c0f7-c14d-4f64-b15f-79b1eebde0f7"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"Think like a man of action and act like man of thought.\"]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 2 2 2 1 1]]\n",
            "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFRw-HoiIhwl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxf8qbbMIjtH"
      },
      "source": [
        "위의 리스트는 count 값을, 아래 딕셔너리는 각 단어들마다의 인덱스를 의미한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Km-byBIqTd",
        "outputId": "149b67e2-52a3-43b3-a383-04e444cd3fe9"
      },
      "source": [
        "vector = CountVectorizer(stop_words='english')          # 영어에 대한 불용어를 제거해준다.\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 2 2 1 1]]\n",
            "{'think': 4, 'like': 2, 'man': 3, 'action': 1, 'act': 0, 'thought': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52NVOyXVKmtd",
        "outputId": "e8b2562e-d60c-4ebd-b12d-e4c90b185efb"
      },
      "source": [
        "corpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 1 1 1 1 1 1 1 1]]\n",
            "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1kN3VOK1u9"
      },
      "source": [
        "하지만 한글은 단순히 공백으로만 구분하기에는 문제가 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9r4b1LtK42-",
        "outputId": "6f3dde58-cb90-4a25-8eac-aa6d65ad9fa7"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "tagger = Mecab()\n",
        "\n",
        "corpus = \"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"\n",
        "tokens = tagger.morphs(re.sub(\"(\\.)\", \"\", corpus))\n",
        "\n",
        "vocab = {}\n",
        "bow = []\n",
        "\n",
        "for tok in tokens:\n",
        "    if tok not in vocab.keys():\n",
        "        vocab[tok] = len(vocab)\n",
        "        bow.insert(len(vocab)-1, 1)\n",
        "    else:\n",
        "        index = vocab.get(tok)\n",
        "        bow[index] = bow[index] + 1\n",
        "\n",
        "print(bow)\n",
        "print(vocab)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1D5wuERLp7f"
      },
      "source": [
        "# 9. 문서 단어 행렬 (DTM)\n",
        "```DTM```은 Document Term Matrix의 준말로, 문서에 등장하는 여러 단어들의 빈호를 행렬로 표현한다. 즉, **각 문서에 대한 BoW를 하나의 행렬로 표현한 것이다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGH5QcZoL5VN",
        "outputId": "4e8b306d-c02f-4c25-b0df-43feecf66a98"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"Think like a man of action and act like man of thought.\",\n",
        "          \"Try not to become a man of success but rather try to become a man of value.\",\n",
        "          \"Give me liberty, or give me death.\"\n",
        "          ]\n",
        "\n",
        "vector = CountVectorizer(stop_words='english')\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 0 0 2 2 0 1 1 0 0]\n",
            " [0 0 0 0 0 2 1 0 0 2 1]\n",
            " [0 0 1 1 0 0 0 0 0 0 0]]\n",
            "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "QtCV_NaHMna5",
        "outputId": "0882256f-07c3-4a07-e74c-404e7c2743a7"
      },
      "source": [
        "# 보기 편하게 다시 출력\n",
        "import pandas as pd\n",
        "\n",
        "columns = []\n",
        "# k: key, v: value\n",
        "# 뒤의 인덱스 값을 기준으로 정렬.\n",
        "for k, v in sorted(vector.vocabulary_.items(), key=lambda item:item[1]):\n",
        "    columns.append(k)\n",
        "\n",
        "df = pd.DataFrame(bow.toarray(), columns=columns)\n",
        "df"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>death</th>\n",
              "      <th>liberty</th>\n",
              "      <th>like</th>\n",
              "      <th>man</th>\n",
              "      <th>success</th>\n",
              "      <th>think</th>\n",
              "      <th>thought</th>\n",
              "      <th>try</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   act  action  death  liberty  like  man  success  think  thought  try  value\n",
              "0    1       1      0        0     2    2        0      1        1    0      0\n",
              "1    0       0      0        0     0    2        1      0        0    2      1\n",
              "2    0       0      1        1     0    0        0      0        0    0      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJXLPyXNRu0"
      },
      "source": [
        "# 10. 어휘 빈도-문서 역빈도(TF-IDF) 분석\n",
        "```TF-IDF```(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법이다. 사용 방법은 우선 DTM을 만든 후, TF-IDF 가중치를 부여한다.     \n",
        "* 단순히 빈도수가 높은 단어가 핵심어가 아닌, 특정 문서에서만 집중적으로 등장할 때 해당 단어가 문서의 주제를 잘 담고 있는 핵심어라고 가정한다.\n",
        "* 특정 문서에서 특정 단어가 많이 등장하고 그 단어가 다른 문서에서 적게 등장할 때, 그 단어를 특정 문서의 핵심어로 간주한다.\n",
        "* 어휘 빈도-문서 역빈도는 어휘 빈도와 역문서 빈도를 곱해 계산이 가능하다.\n",
        "* **어휘 빈도**는 **특정 문서에서 특정 단어**가 많이 등장하는 것을 의미한다.\n",
        "* **역문서 빈도**는 다른 문서에서 등장하지 않는 단어 빈도를 의미한다.\n",
        "* **어휘 빈도-문서 역빈도(TF-IDF)**는 ```sklearn```의 ``tfidfvectorizer````를 이용한다. 앞서 계산한 빈도수를 입력하여 ```TF-IDF```로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1W5D8XgPuEf",
        "outputId": "afb5ce03-38e9-406b-e303-800a65e649d5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english').fit(corpus)\n",
        "\n",
        "print(tfidf.transform(corpus).toarray())\n",
        "print(tfidf.vocabulary_)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.311383   0.311383   0.         0.         0.62276601 0.4736296\n",
            "  0.         0.311383   0.311383   0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.52753275\n",
            "  0.34682109 0.         0.         0.69364217 0.34682109]\n",
            " [0.         0.         0.70710678 0.70710678 0.         0.\n",
            "  0.         0.         0.         0.         0.        ]]\n",
            "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnyZwfZ8QI9v"
      },
      "source": [
        "좀 더 보기 편한 형태로 만들기 위해 데이터프레임 형태로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "SFyc4NvaQOvV",
        "outputId": "2ee4e6b8-baa5-4e0d-b951-034af8a9056b"
      },
      "source": [
        "columns = []\n",
        "# k: key, v: value\n",
        "# 뒤의 인덱스 값을 기준으로 정렬.\n",
        "for k, v in sorted(tfidf.vocabulary_.items(), key=lambda item:item[1]):\n",
        "    columns.append(k)\n",
        "\n",
        "df = pd.DataFrame(tfidf.transform(corpus).toarray(), columns=columns)\n",
        "df"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>death</th>\n",
              "      <th>liberty</th>\n",
              "      <th>like</th>\n",
              "      <th>man</th>\n",
              "      <th>success</th>\n",
              "      <th>think</th>\n",
              "      <th>thought</th>\n",
              "      <th>try</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.311383</td>\n",
              "      <td>0.311383</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.622766</td>\n",
              "      <td>0.473630</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.311383</td>\n",
              "      <td>0.311383</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.527533</td>\n",
              "      <td>0.346821</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693642</td>\n",
              "      <td>0.346821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        act    action     death  ...   thought       try     value\n",
              "0  0.311383  0.311383  0.000000  ...  0.311383  0.000000  0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.693642  0.346821\n",
              "2  0.000000  0.000000  0.707107  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1V7Q6WQQht6"
      },
      "source": [
        "위와 같이 중요도를 출력해준다.      \n",
        "\n",
        "\n",
        "count가 중요한 영역이 있고 TF-IDF가 중요한 영역이 있다. 분석하려는 데이터와 목적에 맞게 잘 선택하면 된다."
      ]
    }
  ]
}
