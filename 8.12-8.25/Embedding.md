Word Embedding
===
> ### Reference
> 이기창, 『한국어 임베딩-자연어 처리 모델의 성능을 높이는 핵심 비결, Word2Vec에서 ELMo, BERT까지, 에이콘』
# 1. 임베딩이란?
`Embedding`이란, 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다.    
단어나 문장 각각을 벡터로 변환해 벡터 공간으로 끼워 넣는다(embed)의 의미에서 임베딩이라는 이름이 붙여졌다.

## 1.1. 임베딩의 역할
임베딩의 역할로는 아래와 같다.
>1. 단어/문장 간 관련도 계산
>2. 의미적/문법적 정보 함축
>3. 전이 학습    

위 세 가지의 역할에 대하여 간단하게 알아보자.
### 1.1.1. 단어/문장 간 관련도 계산
가장 대표적인 임베딩 기법은 `Word2Vec`이라는 기법이다. `Word2Vec`은 **단어들을 벡터로 바꾸는 방법**이다. 예를 들어 보자.    

단어 '희망'이 있다. 이 '희망'이라는 단어 자체로는 컴퓨터는 이해하지 못한다. 따라서 이 단어를 벡터화하여 컴퓨터가 이해할 수 있는 형태로 바꿔주어야 한다. 
그래서 우리는 `Word2Vec`을 이용하여 100차원으로 학습해봤다.
```
[-0.00209 -0.003918 ... -0.04975 0.09300]
```
위 수식은 '희망'의 벡터 수식이다. 100차원으로 학습시켰기 때문에 100개의 숫자를 갖고 있다.    
우리는 단어를 벡터로 임베딩하는 동시에 **벡터들 사이의 유사도를 계산하는 일이 가능해진다.** 또한 **벡터 공간을 기하학적으로 나타낸 시각화 역시 가능하다.**    
하지만 100차원의 단어 벡터들을 시각화하기란 쉬운 일이 아니다. 이럴 때는 차원 축소를 사용하여 시각화할 수 있다. 대표적인 방법으로는 `t-SNE` 기법이 있다.

### 1.1.2. 의미/문법 정보 함축
단어를 임베딩함으로써 벡터간 유사도 계산, 벡터 공간 시각화 뿐만 아니라 **사칙 연산**도 가능하다. 단어 벡터간 덧셈/뺄셈을 통해 단어들 사이의 의미적,
문접적 관계를 도출해낼 수 있다.
```
단어1 + 단어2 - 단어3 = 단어4    
아들 + 딸 - 소녀 = 소년
```
이렇게 단어 임베딩을 평가하는 방법을 `단어 유추 평가`라고 부른다.

### 1.1.3. 전이 학습
임베딩은 다른 딥러닝 모델의 입력값으로 자주 쓰인다. 문서 분류를 위한 딥러닝 모델을 만든다고 할 때 단어 임베딩은 강력한 힘을 발휘한다. 품질 좋은 임베딩을 쓰면 
문서 분류 정확도와 학습 속도가 올라간다. **이렇게 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 전이 학습이라고 한다.**    

전이 학습 모델은 0에서 시작하지 않는다. 대규모 말뭉치를 활용해 임베딩을 미리 만들어둔다. 임베딩에는 의미적, 문법적 정보 등이 들어있다. 

***
# 2. 임베딩 기법
임베딩 기법은 초기에는 말뭉치의 통계량을 직접적으로 활용했다. 대표적인 기법으로는 LSA(잠재 의미 분석)이 있다. 
* TF-IDF 행렬
* 단어-문맥 행렬
* 점별 상호 정보량 행렬

최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 떠오르고 있다. 뉴럴 네트워크는 구조가 유연하고 표현력이 풍부하기 때문에 자연어의 문한한 문맥을 상당 부분 학습할 수 있다.

## 2.1. 단어 수준 임베딩 기법
2017년 이전의 임베딩 기법들은 거의 단어 수준 모델들이었다.
* Word2Vec
* GloVe
* FastText
* Swivel
* NPML

`단어 임베딩 기법`은 각각의 벡터에 **해당 단어의 문맥적 의미를 함축한다.** 하지만 **동음이의어 분간이 어렵다.** 
형태가 같으면 같은 단어로 보고 모든 문맥 정보를 해당 단어에 투영하기 때문이다.

## 2.2. 문장 수준 임베딩 기법
* ELMo
* BERT
* GPT

문장 수준 임베딩 기법은 개별 단어가 아닌 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습 효과가 더 좋다.

## 2.3. 룰 → 엔드투엔드 → 프리트레인/파인 튜닝
>### 룰
>* 사람이 feature(입력값)을 직접 뽑는다.
>* 언어학적인 규칙을 모델에 알려준다.

>### 엔드투엔드
>* 딥러닝 모델은 입력과 출력 사이의 관계를 잘 근사하기 때문에 모델에 규칙을 직접 알려줄 필요가 없다.
>* 데이터를 통째로 모델에 넣고 입출력 사이의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 한다.
>* Seq2Seq 

>### 프리트레인/파인 튜닝
>* 대규모 말뭉치로 임베딩을 만든다. (프리트레인) -> 말뭉치의 의미적, 문법적 맥락이 포함되어 있다.
>* 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만든다.
>* 우리가 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트 한다.(파인 튜닝, 전이 학습)
>* ELMo, GPT, BERT(문장 수준 임베딩)

## 2.4. 다운스트림 태스크/업스트림 태스크
* `downstream task`: 품사 판별, 개체명 인식, 의미역 분석 등
* `upstream task`: 다운스트림 태스크에 앞서 해결해야할 과제. 단어/문장 임베딩을 프리트레인하는 작업

임베딩 품질이 좋아야 문제를 제대로 풀 수 있다. 따라서 upstream task를 제대로 처리해야 한다.

>## 다운스트림 태스크
>* 품사 판별
>* 문장 성분 분석
>* 의존 관계 분석
>* 의미역 분석
>* 상호 참조 해결

## 2.5. 임베딩 기법
* 행렬 분해
* 예측
* 토픽 기반

> ### 행렬 분해 기반 방법
>`행렬 분해 기반 방법`은 말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법이다. 분해한 후, 하나의 행렬만 쓰거나 둘을 더하거나 
이어 붙여 임베딩으로 사용한다. 대표적으로 `GloVe`, `Swivel` 등이 있다.

>### 예측 기반 방법
>어떤 단어 주변에 특정 단어가 나타날지 예측하거나 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나 문장 내 일부 단어를 지우고 해당 단어가 무엇일지
>맞추는 과정에서 학습하는 방법이다. 대표적으로 `Word2Vec`, `FastText`, `BERT`, `ELMo`, `GPT` 등이 있다.

>### 토픽 기반 방법
>주어진 문서에 잠재된 주제를 추론하는 방식이다. 잠재 디리클레 할당, 즉 `LDA`가 대표적이다. 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률벡터로 반환한다.



# 3. 용어 정리
* `토큰`: 다루게 될 데이터의 최소 단위
* `문장`: 토큰의 집합
* `문서`: 문장의 집합
* `말뭉치`: 문서의 집합
* `Tokenize`: 문장을 토큰으로 분석하는 과정
* `어휘 집합`: 말뭉치에 있는 모든 문서를 문장으로 나누고 여기에 토크나이즈르 실시한 후 중복을 제거한 토큰들의 집합


