{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab03.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMcCbZ6rv5WOIhDdao8+z58",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kookeej/DILAB/blob/main/7.1-7.7/lab03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WiZ2ZoB-L1T"
      },
      "source": [
        "Lab03 - Deeper Look at Gradient Descent\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIFtyYmRBrD6"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g73TMqSP-M56"
      },
      "source": [
        "# Dataset\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1)\n",
        "\n",
        "# learning rate 설정\n",
        "lr = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKuRvPSlAs_z"
      },
      "source": [
        "* 이번 강의에서는 H(x) = Wx로 간소화한다. 먼저 모델을 학습시키기 전에 모델을 초기화한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PMPlthPA7vz"
      },
      "source": [
        "nb_epochs = 10\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    # H(x) 계산\n",
        "    hypothesis = x_train * W\n",
        "\n",
        "    # cost gradient 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
        "\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, W.item(), cost.item()\n",
        "    ))\n",
        "\n",
        "    # cost gradient로 H(x) 개선\n",
        "    W = W - lr * gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8fVMPn8DgS6"
      },
      "source": [
        "Pytorch에서는 Gradient descent를 편리하게 사용할 수 있도록 ```torch.optim```을 제공한다. torch.optim을 사용하기 위해서는,\n",
        "* 시작할 때, Optimizer를 정의한다.\n",
        "* optimizer.zero_grad()로 gradient를 0으로 초기화한다. pytorch에서는 gradient값들을 backward 하면서 계속 더해주기 때문에 0으로 계속해서 초기화시켜야 우리가 원하는 결과를 얻을 수 있다.\n",
        "* cost.backward()로 gradient를 계산한다. cost function을 미분해서 각 변수들의 gradient를 채운다.\n",
        "* optimizer.step()으로 gradient descent를 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJFIkkkTE-Sq"
      },
      "source": [
        "다음은 ```torch.optim```을 사용한 Gradient descent의 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACCRTLghFFNy"
      },
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = torch.optim.SGD([W], lr=0.15)\n",
        "\n",
        "nb_epochs = 10\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    # H(x) 계산\n",
        "    hypothesis = x_train * W\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    print('Epoch {:4d}/{} W: {:.3f} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, W.item(), cost.item()\n",
        "    ) )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJyAWuvIF4JS"
      },
      "source": [
        "주어진 학습 데이터에 대해 가장 이상적인 W값은 1이었다. 학습하면서 점점 W가 1에 수렴하고 Cost가 점점 줄어드는 것을 확인할 수 있다."
      ]
    }
  ]
}