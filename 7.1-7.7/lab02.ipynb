{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOUg5wByNnAWNqggHoxzf4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kookeej/DILAB/blob/main/7.1-7.7/lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4JNeBI6rh8U"
      },
      "source": [
        "Lab02 - Linear regression\n",
        "===\n",
        "* Data definition\n",
        "* Hypothesis\n",
        "* Compute loss\n",
        "* Gradient descent algorithm\n",
        "    \n",
        "우리는 학습 데이터를 먼저 정의하고 학습할 Hypothesis를 구현한다. 그리고 학습 데이터를 이용하여 연속적으로 모델을 개선시킨다. 그러기 위해서 Gradient descent algorithm을 통해서 loss를 계산하고, 이를 최소화하는 방향으로 학습한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtTzJKVloS3b"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndJ8GD3Dott7"
      },
      "source": [
        "# 1. Data Definition\n",
        "먼저 training data 중 x값과 y값을 각기 다른 tensor에 저장한다. x는 입력값이고 y는 출력값이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBCV50LiobUU"
      },
      "source": [
        "# Data definition\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpJXH3Iisick"
      },
      "source": [
        "# 2. Hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XkoYM1Yo9Xs"
      },
      "source": [
        "```linear regression```은 학습 데이터와 가장 적합한 하나의 직선(모델)을 찾는 일이다. 이러한 직선은 아래와 같은 함수로 나타낼 수 있다.     \n",
        "```\n",
        "H(x) = Wx + b\n",
        "```\n",
        "W는 ```weight```, b는 ```bias```라고 한다. 따라서 hypothesis를 정의하기 위해서는 먼저 W와 b를 정의해야 한다.    \n",
        "W와 b를 모두 0으로 초기화한다면 어떨까? 항상 출력은 0이 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUK_t9AUpywr"
      },
      "source": [
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "hypothesis = x_train * W + b"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ9xRRfBqBwS"
      },
      "source": [
        "* ```requires_grad=True```를 사용하여 W와 b를 학습시킬 것을 명시한다. 해당 tensor에 대한 계산 모두 tracking하여 기울기를 구해준다.\n",
        "* ```torch.zeros()```는 0으로된 tensor를 말한다. ```torch.zeros(1, requires_grad=True)```는 0으로만 이루어진 (1,) 크기의 tensor를 말한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7QWkP4-rVSj"
      },
      "source": [
        "여기까지 data와 model 정의가 모두 끝났다. 이제 학습을 시킨다. 학습을 시키기 위해서는 우리의 모델이 얼마나 정답과 가까운지를 알아야 한다. 학습을 시키기 전에 ```cost function```를 정의할 것이고 ```cost```값을 최소화시키는 것이 우리의 목표다.    \n",
        "cost function의 형태는 아래와 같다.        \n",
        "\n",
        "![cost_function](https://render.githubusercontent.com/render/math?math=cost%28W%2C%20b%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum%5Em_%7Bi%3D1%7D%20%5Cleft%28%20H%28x%5E%7B%28i%29%7D%29%20-%20y%5E%7B%28i%29%7D%20%5Cright%29%5E2&mode=display)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XrWT3f_t0fs"
      },
      "source": [
        "cost = torch.mean((hypothesis - y_train) ** 2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnqNnUBkuiYz"
      },
      "source": [
        "# 3. Gradient Descent Algorithm\n",
        "우리가 구한 ```cost(W, b)```을 통해 cost function을 최소화시키는 W, b값을 찾을 것이다. cost(W, b)는 밥그릇과 같은 모양을 하고 있기 때문에 경사도(기울기)가 0이 되는 지점을 찾으면 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLLOu7hDvTYE"
      },
      "source": [
        "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "optimizer.zero_grad()       # zero_grad()로 gradient를 초기화한다.  ?????왜??????\n",
        "cost.backward()             # backward()로 gradient를 계산한다. ?????????왜??????\n",
        "optimizer.step()            # step을 통해 개선해 나간다."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CMXwVuy3YaL"
      },
      "source": [
        "* pytorch에서는 왜 항상 optimizer.zero_grad()를 해줄까?    \n",
        "        Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문이다. 따라서 우리는 backpropagation을 하기 전에 gradient를 항상 zero로 만들어주고 시작해야한다.\n",
        "        매번 cost.backward()를 호출할 때, 매번 gradient를 더해주는 것으로 설정되어 있다. 따라서 학습 loop를 돌 때, 정상적으로 학습이 이루어지기 위해서는 한 번의 학습이 완료되어지면 gradient를 항상 0으로 만들어주어야 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc1QI4L617Ns"
      },
      "source": [
        "이제 완성된 코드를 모두 살펴본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-we5JSa61-az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b249f63-9283-43a1-c130-6b76188e6a8e"
      },
      "source": [
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    hypothesis = x_train * W + b\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
            "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
            "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
            "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
            "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
            "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
            "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
            "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
            "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
            "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
            "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFIUP2Th8mmM"
      },
      "source": [
        "* epoch(에포크)란?    \n",
        "        한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass + backward pass과정을 거친 것을 말한다. \n",
        "        즉, 전체 데이터 셋에 대해 한 번의 학습을 완료한 상태이다. \n",
        "        위 코드는 총 1000번의 학습을 한다는 뜻이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9v-0d2E5arb"
      },
      "source": [
        "반복적으로 학습하며 최적의 W와 b값을 찾아나간다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRou6ugR2gm6"
      },
      "source": [
        "* 한번만    \n",
        "    1. 데이터 정의\n",
        "    2. Hypothesis 초기화\n",
        "    3. Optimizer 정의\n",
        "\n",
        "* 반복     \n",
        "    1. Hypothesis 예측\n",
        "    2. Cost 계산\n",
        "    3. Optimizer로 학습"
      ]
    }
  ]
}