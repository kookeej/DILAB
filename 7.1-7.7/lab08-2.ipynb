{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab08-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJGYmCUWNgI1/FNBfFJkxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kookeej/DILAB/blob/main/7.1-7.7/lab08-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27NfiixsWjKz"
      },
      "source": [
        "Lab08-2 - Multi Layer Perceptron\n",
        "===\n",
        "하나의 퍼셉트론만으로는 ```XOR```문제를 해결할 수 없지만, ```multi layer perceptron```으로 그 문제를 해결할 수 있다. 이 ```MLP```는 ```backpropagation```을 통해 학습한다.        \n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4bTVM8ErtMGl3FKHPhnGN8SyGXFhJ-HS1Ow&usqp=CAU)     \n",
        "이처럼 두 개의 직선으로 XOR문제를 해결할 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbndb4dEW1tk"
      },
      "source": [
        "# 1. Backpropagation\n",
        "입력 x가 들어왔을 때, ```Neural Network```를 통해 출력값을 구한다. 이 출력값과 실제값의 차이인 ```cost(또는 loss)```의 Weight에 대한 미분값을 통해 이 cost값을 최소화시킬 수 있도록 ```weight```를 ```update```하는 방식을 ```backpropagation```이라고 한다.     \n",
        "다시 쉽게 말하면, Neural network의 결과가 잘못된 경우, 역전파를 통해 그 Error에 대한 Weight와 Bias를 수정하는 알고리즘이다.\n",
        "    \n",
        "하지만 많은 Layer를 가진 NN에서는 문제점이 발생한다. Layer수가 많아지게 되면, error의 의미가 점점 작아져 제대로 학습시키기가 어려워지기 때문이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS_5DKMeSqrq"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RITp445pqXTj"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8JipxkJS25R"
      },
      "source": [
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CEC0xZBTJHU"
      },
      "source": [
        "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
        "linear2 = torch.nn.Linear(2, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3XY7zvfTQ7R"
      },
      "source": [
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxJq7Z71ToPL"
      },
      "source": [
        "# define cost and optimizer\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mgQ_bvOT-QV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ecb84ca-db35-4ca4-f882-7afe4ca54bb8"
      },
      "source": [
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7434073090553284\n",
            "100 0.693165123462677\n",
            "200 0.6931577920913696\n",
            "300 0.6931517124176025\n",
            "400 0.6931463479995728\n",
            "500 0.6931411027908325\n",
            "600 0.6931357383728027\n",
            "700 0.6931295394897461\n",
            "800 0.6931220889091492\n",
            "900 0.6931126117706299\n",
            "1000 0.6930999755859375\n",
            "1100 0.693082332611084\n",
            "1200 0.6930569410324097\n",
            "1300 0.6930191516876221\n",
            "1400 0.6929606199264526\n",
            "1500 0.6928660273551941\n",
            "1600 0.6927032470703125\n",
            "1700 0.6923960447311401\n",
            "1800 0.6917302012443542\n",
            "1900 0.6899654269218445\n",
            "2000 0.6838315725326538\n",
            "2100 0.6561667323112488\n",
            "2200 0.43110138177871704\n",
            "2300 0.13489334285259247\n",
            "2400 0.06630435585975647\n",
            "2500 0.042168181389570236\n",
            "2600 0.030453868210315704\n",
            "2700 0.023665912449359894\n",
            "2800 0.01927776448428631\n",
            "2900 0.016224022954702377\n",
            "3000 0.013983809389173985\n",
            "3100 0.012273931875824928\n",
            "3200 0.010928118601441383\n",
            "3300 0.009842473082244396\n",
            "3400 0.008949032984673977\n",
            "3500 0.008201321586966515\n",
            "3600 0.007566752843558788\n",
            "3700 0.007021686062216759\n",
            "3800 0.006548595614731312\n",
            "3900 0.006134253926575184\n",
            "4000 0.005768344737589359\n",
            "4100 0.0054430365562438965\n",
            "4200 0.005151905119419098\n",
            "4300 0.004889918025583029\n",
            "4400 0.0046528722159564495\n",
            "4500 0.004437457304447889\n",
            "4600 0.004240859299898148\n",
            "4700 0.004060687031596899\n",
            "4800 0.003895031288266182\n",
            "4900 0.0037421947345137596\n",
            "5000 0.0036007347516715527\n",
            "5100 0.003469479735940695\n",
            "5200 0.0033473046496510506\n",
            "5300 0.0032333978451788425\n",
            "5400 0.0031268750317394733\n",
            "5500 0.0030270610004663467\n",
            "5600 0.0029333550482988358\n",
            "5700 0.0028452035039663315\n",
            "5800 0.00276215560734272\n",
            "5900 0.002683777129277587\n",
            "6000 0.0026096487417817116\n",
            "6100 0.0025394847616553307\n",
            "6200 0.0024729417636990547\n",
            "6300 0.0024097643326967955\n",
            "6400 0.002349698217585683\n",
            "6500 0.0022925634402781725\n",
            "6600 0.002238075714558363\n",
            "6700 0.002186085097491741\n",
            "6800 0.0021364721469581127\n",
            "6900 0.002089011948555708\n",
            "7000 0.00204361486248672\n",
            "7100 0.0020001311786472797\n",
            "7200 0.0019584265537559986\n",
            "7300 0.0019184107659384608\n",
            "7400 0.0018799942918121815\n",
            "7500 0.0018430722411721945\n",
            "7600 0.0018075549742206931\n",
            "7700 0.0017733527347445488\n",
            "7800 0.0017404207028448582\n",
            "7900 0.0017087137093767524\n",
            "8000 0.001678097527474165\n",
            "8100 0.0016485571395605803\n",
            "8200 0.0016200175741687417\n",
            "8300 0.0015924491453915834\n",
            "8400 0.0015657918993383646\n",
            "8500 0.0015400308184325695\n",
            "8600 0.0015150614781305194\n",
            "8700 0.0014909137971699238\n",
            "8800 0.0014674977865070105\n",
            "8900 0.001444813678972423\n",
            "9000 0.0014228165382519364\n",
            "9100 0.0014014765620231628\n",
            "9200 0.0013806892093271017\n",
            "9300 0.0013606036081910133\n",
            "9400 0.0013410557294264436\n",
            "9500 0.001322030322626233\n",
            "9600 0.001303557539358735\n",
            "9700 0.001285637030377984\n",
            "9800 0.0012681199004873633\n",
            "9900 0.0012511102249845862\n",
            "10000 0.0012345188297331333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPrXvmtxtnor"
      },
      "source": [
        "* cost값이 정상적으로 잘 줄어들고 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wplTdcKZtJdR",
        "outputId": "47597c36-4f5b-4c33-bbb1-afe44062d8b2"
      },
      "source": [
        "with torch.no_grad():\n",
        "    hypothesis = model(X)\n",
        "    predicted = (hypothesis > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis:', hypothesis.detach().cpu().numpy(), '\\nCorrect:', predicted.detach().cpu().numpy(), '\\nAccuracy:', accuracy.item())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hypothesis: [[0.00106364]\n",
            " [0.99889404]\n",
            " [0.99889404]\n",
            " [0.00165861]] \n",
            "Correct: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4AAl0LtiCZ"
      },
      "source": [
        "* MLP를 통해 정확도가 눈에 띄게 향상된 것을 확인할 수 있다."
      ]
    }
  ]
}